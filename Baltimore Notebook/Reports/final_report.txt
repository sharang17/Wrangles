EVALUATION
========================


Ranking
-------------------------

In order to rigorously evaluate our ranking data we turned to several distinct datasets and approaches. I will begin with the most quantitative of our approaches and move towards the least, detailing the relative merits and demerits, as well as the unique challenges posed by each, as I move along.

As part of their open data platform Baltimore provides anonymized 311 service data. A 311 service is an interface the city provides for non-emergency citizen reporting. Through 311 residents can flag problems for the city to rectify --in effect acting as a distributed citizen sensor network. Each row of the dataset captures this sensor data as an incident type, the neighborhood to which it applies, a start, status and due date, and an outcome.

Though the data is mostly well manicured the neighborhood schema which Baltimore employs differs from the community statistical area (CSA) model that the BNIA organization uses. Because the bulk of our statistics correspond to these CSA areas the subneigborhoods of the 311 dataset had to be aggregated into the CSA format. More subtly, the dataset actually uses two different timestamp formats within each row -- swapping the date and month. Close examination revealed several cases that ended before they started, or that had what appeared to be invalid month codes.

After cleaning the data of these irregularities we proceeded to our analysis. Our first approach was to use simple frequency counts to measure the dysfunction of each neighborhood. The assumption here would be that neighborhoods with more problems will have correspondingly higher 311 call totals. As it turns out, there is a negative correlation, as we expected. Unfortunately, it is a rather weak one at best. This first pass yielded a correlation of -0.20023 and a relatively high p value of .14.

Perhaps then our first thought was too naive. 311 is a very general service that covers a huge array of incidents and not all of them are good proxies for quality. So next we sorted the categories by popularity and removed 311 data that we considered irrelevant to neighborhood quality and only kept measures related to health, housing, and pests. For instance, one such category is property sanitation. A 311 incident labeled "HCD-Sanitation Property" is used to denote an instance where a resident complains about the state of another residents property. If a neighbor's property were strewn with garbage or in disrepair then this HCD code would apply. Now operating under the assumption that certain classes of 311 call better mapped to neighborhood quality we again did the frequency tallying. The new dataset was only slightly less disappointing. This time we squeaked out a correlation of -.25 and a P value of .061.

So then, is using the 311 data to evaluate our model an idea dead in the water? Not yet. Our third approach utilized the time delta between when a case was opened and closed. The assumption here is that perhaps neighborhoods that are worse quality will also get lower quality service as the city might try to economize on labor, prioritize high value areas, and also satisfy it's most wealthy, and therefor influential, residents.

In order to get a handle on how the data looked we wanted to compute median and mean values for each incident type for each neighborhood. From there we could examine the variance between the median and mean values across all neighborhoods. The intuition was that we could focus on areas of highvariance.

We began by binning all of the 311 calls on a neighborhood basis. This means that we mapped row numbers to incident type bins on a neighborhood by neighborhood basis. We then calculated the median and mean for each incident bucket.

This was not as simple as it seems. For one, we had to keep a raw count so we could validate that our statistics were meaningful. If a particular neighborhood or incident category had few meaningful samples across the board then it was discarded after a case by case examination. More troublingthe question of how we should represent our time deltas. While it made sense to store the data at its most granular resolution -- i.e. accurate to the second -- should we calculate variance there too? This was certainly a test of judgement. If we calculate variance at the level of seconds thenthe variance would be astronomical, but choose too large a timescale and you would see no resolution at all. All differences look small on the timeline to infinity. To this point we tested empirically what sorts of variance we would get at the second, minute, and hour timescale. Variance was astronomical if we used seconds, and very low at the hour end.

What does this mean? Well, perhaps that looking for variance to key us into categories to examine is the wrong approach. Instead, with that thinking in mind we moved on to calculating correlations between all valid incident categories (those that met a minimum count value) and the output of our scoring function.

Here to the interpretation was tricky. There were both positive and negative correlations (some reaching around .45 in either direction). What to make of that is unclear. Does the city prioritize certain kinds of service for some neighborhoods? Maybe, maybe not. It seems more likely that thereis no real preference for neighborhood. Otherwise, would it make sense that 'TRS-Abandoned Vehicle' claims with an aggregate count of near 18000 incidents would be positively correlated with score --i.e. the higher the score the longer above the median time for the category it takes to close an abandoned vehicle case -- while 'TRS-Parking Complaint Commercial Veh. Residential' is negatively correlated? Or that 'HCD-Sanitation Property' is positively correlated (.35) while 'TRM-Illegal Sign Removal' would be negatively correlated (-.29)? Both of these requests seem be beautification
related and therefore high impact and probably high priority IF such a system of priorities did exist. Further, while it may be possible after seeing the data to come up with an ad-hoc rationalization we do not think that we can do so responsibly.

Given all of this, we tried one last approach on the 311 data. We wanted to look at the top 3 neighborhoods as ranked by our baseline model and thebottom three and see whether or not things became more clear. The correlation value ceiling increased significantly but in most cases the corresponding p value naturally increased as well. One particularly tickling insight: the amount of time to close a citizens complaint about a city employeewas significantly correlated with score.
